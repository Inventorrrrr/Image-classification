{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Flatten , Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.nn import relu , sigmoid\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport os\nimport urllib\nimport zipfile\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2022-04-24T07:49:29.677321Z","iopub.execute_input":"2022-04-24T07:49:29.678081Z","iopub.status.idle":"2022-04-24T07:49:29.685144Z","shell.execute_reply.started":"2022-04-24T07:49:29.678035Z","shell.execute_reply":"2022-04-24T07:49:29.684173Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"file_names = []\nclasses = []\nroot_path = '../input/fruit-and-vegetable-image-recognition/train'\nfor folder in os.listdir(root_path):\n    new_path = os.path.join(root_path,folder)\n    for file in os.listdir(new_path):\n        file_names.append(os.path.join(new_path,file))\n        classes.append(folder)\n\ndata = pd.DataFrame({'filenames':file_names,'class':classes})","metadata":{"execution":{"iopub.status.busy":"2022-04-24T07:49:33.254399Z","iopub.execute_input":"2022-04-24T07:49:33.255058Z","iopub.status.idle":"2022-04-24T07:49:33.299483Z","shell.execute_reply.started":"2022-04-24T07:49:33.255018Z","shell.execute_reply":"2022-04-24T07:49:33.298668Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T07:50:50.854438Z","iopub.execute_input":"2022-04-24T07:50:50.855214Z","iopub.status.idle":"2022-04-24T07:50:50.870618Z","shell.execute_reply.started":"2022-04-24T07:50:50.855177Z","shell.execute_reply":"2022-04-24T07:50:50.869441Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_data_gen = ImageDataGenerator(rescale=1./255,\n      rotation_range=40,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\ntest_data_gen = ImageDataGenerator(rescale=1./255)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T07:51:48.298788Z","iopub.execute_input":"2022-04-24T07:51:48.299174Z","iopub.status.idle":"2022-04-24T07:51:48.306415Z","shell.execute_reply.started":"2022-04-24T07:51:48.299117Z","shell.execute_reply":"2022-04-24T07:51:48.305418Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_data = train_data_gen.flow_from_directory(\n    '../input/fruit-and-vegetable-image-recognition/train',\n    batch_size=100 ,\n    target_size=(150,150),\n    class_mode='binary'\n)\n\ntest_data = test_data_gen.flow_from_directory(\n    '../input/fruit-and-vegetable-image-recognition/test' ,\n    batch_size=100 ,\n    target_size=(150,150),\n    class_mode='binary'\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T07:57:20.860061Z","iopub.execute_input":"2022-04-24T07:57:20.860953Z","iopub.status.idle":"2022-04-24T07:57:21.184919Z","shell.execute_reply.started":"2022-04-24T07:57:20.860889Z","shell.execute_reply":"2022-04-24T07:57:21.183623Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pre_trained_model = InceptionV3(include_top=False , weights=None , input_shape=(150,150,3))\nweights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nweights_file = \"inception_v3.h5\"\nurllib.request.urlretrieve(weights_url, weights_file)\n\npre_trained_model.load_weights(weights_file)\n\nfor layer in pre_trained_model.layers:\n    layer.trainable = False\n\nlast_layer = pre_trained_model.get_layer('mixed7')\n\nx = Flatten()(last_layer.output)\nx = Dense(1024, activation='relu')(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(pre_trained_model.input, x)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T07:59:07.828762Z","iopub.execute_input":"2022-04-24T07:59:07.829683Z","iopub.status.idle":"2022-04-24T07:59:11.224671Z","shell.execute_reply.started":"2022-04-24T07:59:07.829610Z","shell.execute_reply":"2022-04-24T07:59:11.223578Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    loss=tf.keras.losses.binary_crossentropy ,\n    optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n    metrics=['acc']\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T07:59:32.109145Z","iopub.execute_input":"2022-04-24T07:59:32.109458Z","iopub.status.idle":"2022-04-24T07:59:32.125006Z","shell.execute_reply.started":"2022-04-24T07:59:32.109426Z","shell.execute_reply":"2022-04-24T07:59:32.123952Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_data ,\n    epochs=2 ,\n    verbose = 1,\n    validation_data=test_data ,\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:15:09.815244Z","iopub.execute_input":"2022-04-24T08:15:09.816031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ , ax = plt.subplots(1,2,figsize = (16,5))\nax[0].plot(history.history['acc'])\nax[0].plot(history.history['val_acc'])\nax[0].set_title(\"Accurecy\")\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title(\"Loss\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:12:19.765530Z","iopub.execute_input":"2022-04-24T08:12:19.766683Z","iopub.status.idle":"2022-04-24T08:12:20.225606Z","shell.execute_reply.started":"2022-04-24T08:12:19.766584Z","shell.execute_reply":"2022-04-24T08:12:20.224449Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"cab_img = tf.keras.preprocessing.image.load_img('../input/fruit-and-vegetable-image-recognition/train/cabbage/Image_11.jpg' , target_size=(150,150,3))\ncab_img = tf.keras.preprocessing.image.img_to_array(cat_img)\ncab_img = cat_img / 255.0\nplt.imshow(cat_img)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:14:00.042842Z","iopub.execute_input":"2022-04-24T08:14:00.043145Z","iopub.status.idle":"2022-04-24T08:14:00.371370Z","shell.execute_reply.started":"2022-04-24T08:14:00.043115Z","shell.execute_reply":"2022-04-24T08:14:00.370356Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"import tensorflow as tf\n","metadata":{}},{"cell_type":"code","source":"train_data.class_indices","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:14:26.964924Z","iopub.execute_input":"2022-04-24T08:14:26.965270Z","iopub.status.idle":"2022-04-24T08:14:26.974429Z","shell.execute_reply.started":"2022-04-24T08:14:26.965238Z","shell.execute_reply":"2022-04-24T08:14:26.973485Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.predict(cab_img[tf.newaxis , ])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}