{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n#Ploting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Preprocessing\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n#model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Dropout,Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n\n#metrics\nfrom sklearn.metrics import classification_report\n\n#Transfer Learning\nfrom tensorflow.keras.applications import VGG16","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:47:35.850834Z","iopub.execute_input":"2022-04-24T06:47:35.851687Z","iopub.status.idle":"2022-04-24T06:47:35.859643Z","shell.execute_reply.started":"2022-04-24T06:47:35.851638Z","shell.execute_reply":"2022-04-24T06:47:35.858661Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"file_names = []\nclasses = []\nroot_path = '../input/fruit-and-vegetable-image-recognition/train'\nfor folder in os.listdir(root_path):\n    new_path = os.path.join(root_path,folder)\n    for file in os.listdir(new_path):\n        file_names.append(os.path.join(new_path,file))\n        classes.append(folder)\n\ndata = pd.DataFrame({'filenames':file_names,'class':classes})","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:49:12.959637Z","iopub.execute_input":"2022-04-24T06:49:12.960516Z","iopub.status.idle":"2022-04-24T06:49:13.013189Z","shell.execute_reply.started":"2022-04-24T06:49:12.960478Z","shell.execute_reply":"2022-04-24T06:49:13.012510Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:49:29.707797Z","iopub.execute_input":"2022-04-24T06:49:29.708113Z","iopub.status.idle":"2022-04-24T06:49:29.729084Z","shell.execute_reply.started":"2022-04-24T06:49:29.708081Z","shell.execute_reply":"2022-04-24T06:49:29.728496Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,7))\nsns.countplot(x='class',data=data)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:50:16.514324Z","iopub.execute_input":"2022-04-24T06:50:16.515008Z","iopub.status.idle":"2022-04-24T06:50:17.040258Z","shell.execute_reply.started":"2022-04-24T06:50:16.514974Z","shell.execute_reply":"2022-04-24T06:50:17.039402Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data_gen = ImageDataGenerator(rescale=1./255,\n                             shear_range = 0.2,\n                             zoom_range = 0.2,\n                             rotation_range = 20,\n                             horizontal_flip = True,\n                             vertical_flip = True,\n                             fill_mode = 'nearest')\ntest_data_gen = ImageDataGenerator(rescale=1./255)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:51:24.430477Z","iopub.execute_input":"2022-04-24T06:51:24.430826Z","iopub.status.idle":"2022-04-24T06:51:24.437051Z","shell.execute_reply.started":"2022-04-24T06:51:24.430791Z","shell.execute_reply":"2022-04-24T06:51:24.436208Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_generator = data_gen.flow_from_directory('../input/fruit-and-vegetable-image-recognition/train',\n                                              target_size = (150,150),\n                                              color_mode = 'rgb',\n                                              class_mode = 'categorical',\n                                              shuffle = True)\nvalid_generator = data_gen.flow_from_directory('../input/fruit-and-vegetable-image-recognition/validation',\n                                              target_size = (150,150),\n                                              color_mode = 'rgb',\n                                              class_mode = 'categorical',\n                                              shuffle = True)\ntest_generator = test_data_gen.flow_from_directory('../input/fruit-and-vegetable-image-recognition/test',\n                                              target_size = (150,150),\n                                              color_mode = 'rgb',\n                                              class_mode = 'categorical',\n                                              shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:53:48.966149Z","iopub.execute_input":"2022-04-24T06:53:48.966484Z","iopub.status.idle":"2022-04-24T06:53:49.452514Z","shell.execute_reply.started":"2022-04-24T06:53:48.966448Z","shell.execute_reply":"2022-04-24T06:53:49.451869Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,10))\nind = np.random.randint(3000)\nimg = data.iloc[ind,0]\n\nimg = image.load_img(img)\n\nimg_array = image.img_to_array(img)\n\nimg_array = np.expand_dims(img_array,axis=0)\n\nimg_a = data_gen.flow(img_array)\nplt.subplot(1,4,1)\nplt.imshow(img)\nplt.title(f'Original | {data.iloc[ind,1]}')\ni = 2\nfor im in img_a:\n    plt.subplot(1,4,i)\n    plt.imshow(image.array_to_img(im[0]))\n    plt.title(\"Augmented\")\n    i += 1\n    if i % 5 == 0:\n        break  ","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:55:47.459780Z","iopub.execute_input":"2022-04-24T06:55:47.460064Z","iopub.status.idle":"2022-04-24T06:55:48.611260Z","shell.execute_reply.started":"2022-04-24T06:55:47.460028Z","shell.execute_reply":"2022-04-24T06:55:48.610575Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def show_sample_images(gne): \n    t_dict = gne.class_indices\n    classes = list(t_dict.keys())\n    images,labels = next(gne)\n    plt.figure(figsize=(20,20))\n    for i in range(25):\n        plt.subplot(5,5,i+1)\n        img = images[i]\n        plt.imshow(img)\n        index = np.argmax(labels[i])\n        class_name = classes[index]\n        plt.title(class_name)\n        plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:56:27.089100Z","iopub.execute_input":"2022-04-24T06:56:27.089824Z","iopub.status.idle":"2022-04-24T06:56:27.096212Z","shell.execute_reply.started":"2022-04-24T06:56:27.089787Z","shell.execute_reply":"2022-04-24T06:56:27.095217Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"show_sample_images(train_generator)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:59:51.316181Z","iopub.execute_input":"2022-04-24T06:59:51.316472Z","iopub.status.idle":"2022-04-24T06:59:55.567782Z","shell.execute_reply.started":"2022-04-24T06:59:51.316442Z","shell.execute_reply":"2022-04-24T06:59:55.566673Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss',patience=3,min_delta = 0.0001)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T07:01:58.111318Z","iopub.execute_input":"2022-04-24T07:01:58.111611Z","iopub.status.idle":"2022-04-24T07:01:58.115279Z","shell.execute_reply.started":"2022-04-24T07:01:58.111581Z","shell.execute_reply":"2022-04-24T07:01:58.114534Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(128,(3,3),padding='same',activation='relu',input_shape=(150,150,3)))\nmodel.add(MaxPool2D(2,2))\n\nmodel.add(Conv2D(64,(3,3),padding='same',activation='relu'))\nmodel.add(MaxPool2D(2,2))\n\nmodel.add(Conv2D(64,(3,3),padding='same',activation='relu'))\nmodel.add(MaxPool2D(2,2))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(36,activation='softmax'))\n\n\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T07:03:39.392412Z","iopub.execute_input":"2022-04-24T07:03:39.392866Z","iopub.status.idle":"2022-04-24T07:03:39.494216Z","shell.execute_reply.started":"2022-04-24T07:03:39.392832Z","shell.execute_reply":"2022-04-24T07:03:39.492993Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator,validation_data=valid_generator,epochs=100,callbacks=[early_stop])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T07:03:57.634465Z","iopub.execute_input":"2022-04-24T07:03:57.634791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_generator)\npred_label = np.argmax(predictions,axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_dict = test_generator.class_indices\nclasses = list(t_dict.keys())\nplt.figure(figsize=(25,20))\n\nimages,labels = next(test_generator)\n\nfor i in range(30):\n    plt.subplot(6,5,i+1)\n    img = images[i]\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array,axis=0)\n    prediction = np.argmax(model.predict(img_array))\n    plt.imshow(img)\n    \n    index = np.argmax(labels[i])\n    class_name = classes[index]\n    prediction = model.predict(img_array)\n    plt.title(f'ground truth : {class_name} | prediction {classes[np.argmax(prediction[0],axis=0)]}')\n    plt.axis('off')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}